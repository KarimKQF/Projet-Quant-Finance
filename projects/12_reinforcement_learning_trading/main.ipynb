{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reinforcement Learning for Trading\n",
                "\n",
                "This notebook demonstrates a simple Q-Learning agent learning to trade in a synthetic market environment.\n",
                "\n",
                "## 1. Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from trading_env import TradingEnv\n",
                "from q_learning_agent import QLearningAgent\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Training Loop\n",
                "\n",
                "We train the agent for a number of episodes. The agent explores the environment and updates its Q-Table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EPISODES = 1000\n",
                "env = TradingEnv(max_steps=100)\n",
                "agent = QLearningAgent(state_size=env.state_space_size, action_size=env.action_space_size)\n",
                "\n",
                "portfolio_values = []\n",
                "rewards_history = []\n",
                "\n",
                "for e in range(EPISODES):\n",
                "    state = env.reset()\n",
                "    total_reward = 0\n",
                "    done = False\n",
                "    \n",
                "    while not done:\n",
                "        action = agent.get_action(state)\n",
                "        next_state, reward, done, info = env.step(action)\n",
                "        agent.learn(state, action, reward, next_state, done)\n",
                "        state = next_state\n",
                "        total_reward += reward\n",
                "        \n",
                "    # End of episode\n",
                "    agent.decay_epsilon()\n",
                "    portfolio_values.append(info['net_worth'])\n",
                "    rewards_history.append(total_reward)\n",
                "    \n",
                "    if (e + 1) % 100 == 0:\n",
                "        print(f\"Episode {e+1}/{EPISODES} - Net Worth: {info['net_worth']:.2f} - Epsilon: {agent.epsilon:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Results Visualization\n",
                "Plotting the final portfolio value per episode to see if the agent improves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 6))\n",
                "plt.plot(portfolio_values)\n",
                "plt.title('Agent Net Worth per Episode')\n",
                "plt.xlabel('Episode')\n",
                "plt.ylabel('Net Worth ($)')\n",
                "plt.axhline(y=10000, color='r', linestyle='--', label='Initial Balance')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Run\n",
                "Run one episode with exploration disabled (Greedy policy) to see the learned behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_env = TradingEnv(max_steps=200)\n",
                "state = test_env.reset()\n",
                "done = False\n",
                "agent.epsilon = 0 # Disable exploration\n",
                "\n",
                "history = {'price': [], 'net_worth': []}\n",
                "\n",
                "while not done:\n",
                "    action = agent.get_action(state)\n",
                "    state, _, done, info = test_env.step(action)\n",
                "    history['price'].append(info['price'])\n",
                "    history['net_worth'].append(info['net_worth'])\n",
                "\n",
                "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "ax1.set_xlabel('Step')\n",
                "ax1.set_ylabel('Price', color='tab:blue')\n",
                "ax1.plot(history['price'], color='tab:blue', label='Price')\n",
                "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "ax2.set_ylabel('Net Worth', color='tab:green')\n",
                "ax2.plot(history['net_worth'], color='tab:green', label='Net Worth', linestyle='--')\n",
                "ax2.tick_params(axis='y', labelcolor='tab:green')\n",
                "\n",
                "plt.title('Test Episode Performance')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}